Below is a practical playâ€‘book for dropping an N3/EYE logic tier into CogitareLink without disturbing the existing SHACL/SPARQL toolâ€‘chain.  I treat N3 as Layerâ€¯2bâ€”a rule bundle that sits next to shapes.ttl, is discovered by the same affordance scan, and is executed by an EYEâ€‘powered sandbox.

â¸»

1â€ƒWhere N3 fits in the fourâ€‘layer stack

 layerâ€‘0  context.jsonld         (â‰¤â€¯2â€¯KB, unchanged)
 layerâ€‘1  ontology.ttl           (RDFS/OWL, unchanged)
 layerâ€‘2a shapes.ttl  rules.ttl  (SHACL / sh:JSRule, existing)
 layerâ€‘2b logic.n3               (N3 rules â€ )   â†  **NEW**
 layerâ€‘3  data.jsonld            (instances, unchanged)

â€ â€¯file name convention: logic.n3 or rules.n3 so the AffordanceScanner can patternâ€‘match it.

â¸»

2â€ƒExecution pathway

Step	What happens	Implementation hook
1	Scanner sees ex:hasN3Rule <logic.n3> in the data or shapes graph.	no Python change neededÂ â€“ treat it like any other affordance.
2	Agent fetches the N3 text and calls the tool:	

{
  "name": "reason_over",
  "arguments": {
    "jsonld": "<events.jsonld>",
    "query": null,
    "shapes_turtle": null,
    "n3_rules": "<logic.n3>"      // â† you add this optional key
  }
}
``` |

| 3 | Sandbox detects `n3_rules` and pipelines **EYE**: `eye data.ttl logic.n3 --pass new --quiet`. | Small patch to `reason.sandbox`; 10â€‘20â€¯LOC. |
| 4 | EYE emits derived triples **and** a Proof JSON; sandbox wraps the new triples with `prov:Activity _:b42`. | Reâ€‘use existing `wrap_patch_with_prov()`. |
| 5 | Agent may issue a followâ€‘up SPARQL query (still through `reason_over`) or just read the NL summary. | No change. |

> **Dependency tip:** use the WASM build **eyeâ€‘jsâ€¯v18.16.1 (Aprâ€¯2025)** so the same binary runs in Node, Python (via *pyodide*), or the browser.  [oai_citation:0â€¡GitHub](https://github.com/eyereasoner/eye-js?utm_source=chatgpt.com) [oai_citation:1â€¡Zenodo](https://zenodo.org/records/15299683?utm_source=chatgpt.com)  

---

## 3â€ƒMapping deontic concepts into N3

| Deontic idea | N3 rule sketch (prefixes omitted) |
|--------------|-----------------------------------|
| **Obligation** *Oâ€¯p* | `{ ?plan a ml:PlannedAction. } => { ?plan dl:obliges ex:Perform_p. }.` |
| **Prohibition** *Fâ€¯p* | `{ ?plan ex:contains p. } => { ?plan dl:violates ex:No_p. }.` |
| **CTD rescue** | `{ ?plan dl:violates ex:NoHarm. } => { ?plan dl:obliges ex:AlertStaff. }.` |
| **Priority** | Encode numeric `ex:priority` on rule heads; EYEâ€™s builtâ€‘in `e:n3/compare` lets you prefer the highest. |

EYE can also import your SHACL violation graph and fire CTD rules **after** validation.  A readyâ€‘made converter (`eye-shacl`) compiles shapes to N3, so one engine can do both validation and reasoning if you prefer a single runtimeÂ  [oai_citation:2â€¡GitHub](https://github.com/giacomociti/eye-shacl?utm_source=chatgpt.com).

---

## 4â€ƒFileâ€‘level wiring

```turtle
# data.jsonld  (excerpt, Turtle)
:plan42  a ml:PlannedAction ;
         ex:hasShape   shapes:NoHarmShape ;
         ex:hasN3Rule  n3:NoHarmDeonticRules .

	â€¢	ex:hasN3Rule is a tiny term you add to Layerâ€‘0 so the scanner recognises it.
	â€¢	n3:NoHarmDeonticRules dereferences to /shapes/logic.n3 (Layerâ€¯2b).
	â€¢	The ontology (ontology.ttl) simply declares ex:hasN3Rule rdfs:range owl:Ontology so RDFS inference works.

â¸»

5â€ƒSandbox glue code (Python sketch)

def reason_over(..., n3_rules=None):
    ...
    if n3_rules:
        data_ttl   = jsonld_to_turtle(jsonld)
        rules_path = save_temp(n3_rules, ".n3")
        cmd = ["eye", data_ttl, rules_path, "--pass", "new", "--quiet"]
        derived_ttl = subprocess.check_output(cmd, text=True)
        patch = turtle_to_graph(derived_ttl) - original_graph
        summary = f"ğŸ’¡ EYE derived {len(patch)} triples via {rules_path}"
        return wrap_patch_with_prov(patch, summary)

That is literally the only Python you need besides a helper to convert JSONâ€‘LD â‡† Turtle.

â¸»

6â€ƒProofs & explanation

EYE can output a JSON proof (--nope --repeat=1000 --pass new --quiet --proof proof.json) which:
	â€¢	names every rule fired (IRI or blank node)
	â€¢	records substitutions, premises, and conclusions.

Store the proof file next to the patch URI; when the agent explains a decision it can cite prov:Activity _:b42 owl:hasProof <proof.json> and selectively quote the relevant rule chain.  This gives users a stepâ€‘byâ€‘step audit logâ€”a feature often requested for safety reviewsÂ  ï¿¼.

â¸»

7â€ƒSHACLâ€¯â‡„â€¯N3 synergy
	â€¢	Validate first with SHACL to surface violations.
	â€¢	Feed the violation triples into N3 rules to trigger CTD or priority logic.
	â€¢	Optionally compile selected SHACL shapes into N3 for a singleâ€‘engine run (eyeâ€‘shacl already does the translationÂ  ï¿¼).

Because everything stays in RDF/Turtle, the handâ€‘off is zeroâ€‘cost.

â¸»

8â€ƒPerformance notes
	â€¢	EYE handles tens of thousands of triples in milliseconds for forwardâ€‘chaining; add --quiet --pass new for speed.
	â€¢	WASM build streams into memory, so no Docker layer is required.
	â€¢	Cache compiled rules (logic.n3) just like you cache shapes.ttlâ€”they are immutable Layerâ€‘2 artefacts.

â¸»

TL;DR
	1.	Add a logic.n3 file (Layerâ€¯2b) and link it with ex:hasN3Rule.
	2.	Patch reason_over so that when n3_rules is present the sandbox calls EYE on the data.
	3.	Return the derived triples + prov + optional proof; everything else in CogitareLink (cache, explain, query) remains unchanged.

With ~20Â lines of Python and one extra file per domain you obtain a full deonticâ€‘capable rule engine that complements SHACL, leverages EYEâ€™s WASM portability, and keeps CogitareLink true to its â€œall logic lives in dataâ€ ethos.