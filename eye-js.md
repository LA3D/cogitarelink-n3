Below is a â€œcookbookâ€â€‘level guide for wiring eyeâ€‘js (the WASM build of the EYE N3 reasoner) into CogitareLink.  Follow the order and you can be up and reasoning in <â€¯30â€¯min.

â¸»

1â€ƒPrerequisites

Tool	Recommended version	Why
Node.js	â‰¥â€¯18â€¯LTS (works on 20â€¯too)	eyeâ€‘js ships an ESâ€‘module bundle compiled against modern Node APIs
npm / pnpm / yarn	latest	to install the package
eyeâ€‘js	18.16.4 (Mayâ€¯2025)	current stable; includes SWIâ€‘Prologâ€¯5.0.9 WASM core  ï¿¼

Tip: pin the exact version in package.json so upgrades are explicit.

â¸»

2â€ƒInstall the package

# inside the cogitarelink repo root
npm install eyereasoner@18.16.4 --save           # pnpm add â€¦  or  yarn add â€¦ also fine

The package name is eyereasoner even though the repo is eyeâ€‘js; thatâ€™s what the published artifact is called.   ï¿¼

â¸»

3â€ƒAdd a minimal JS runner

Create js/eye-runner.mjs (ES module so it works in Node without transpilation):

// js/eye-runner.mjs
import { readFileSync } from 'node:fs';
import { stdin } from 'node:process';
import { n3reasoner } from 'eyereasoner';          // <-- eyeâ€‘js API

/** Expect JSON on stdin: { dataPath, rulesPath, queryPath|null } */
const input = JSON.parse(await new Promise(r => {
  let buf = ''; stdin.setEncoding('utf8');
  stdin.on('data', d => buf += d);
  stdin.on('end', () => r(buf));
}));

const data   = readFileSync(input.dataPath,  'utf8');
const rules  = readFileSync(input.rulesPath, 'utf8');
const query  = input.queryPath ? readFileSync(input.queryPath, 'utf8') : undefined;

const result = await n3reasoner(`${data}\n${rules}`, query); // returns N3 string
process.stdout.write(result);

Why JSON over stdin?Â It avoids shellâ€‘escaping nightmares for long N3 strings.

â¸»

4â€ƒPatch reason.sandbox

Add a 10â€‘line branch when n3_rules is supplied:

import json, subprocess, tempfile, uuid, os
from pathlib import Path

def _eye_run(data_ttl: str, rules_n3: str, query_n3: str | None = None) -> str:
    workdir = Path(tempfile.mkdtemp(prefix="eyejs_"))
    data_f  = workdir / "data.ttl"
    rules_f = workdir / "rules.n3"
    query_f = workdir / "query.n3"
    data_f.write_text(data_ttl,  encoding="utf8")
    rules_f.write_text(rules_n3, encoding="utf8")
    if query_n3: query_f.write_text(query_n3, encoding="utf8")

    payload = {
        "dataPath": str(data_f),
        "rulesPath": str(rules_f),
        "queryPath": str(query_f) if query_n3 else None
    }
    out = subprocess.check_output(
        ["node", "js/eye-runner.mjs"],
        input=json.dumps(payload),
        text=True
    )
    return out  # N3 string with *only* the newly derived triples

Then call _eye_run inside the existing reason_over() branch, convert the returned N3 to an rdflib.Graph, diff it against the input graph, wrap with provenance, and youâ€™re done.

â¸»

5â€ƒUpdate the OpenAIâ€‘function spec (already reflected in the doc)

 "properties": {
   "jsonld":      { "type":"string" },
   "shapes_turtle": { "type":"string", "nullable":true },
   "query":         { "type":"string", "nullable":true },
+  "n3_rules":      { "type":"string", "nullable":true }
 }

The sandbox should refuse calls that specify more than one of shapes_turtle, query, or n3_rules.

â¸»

6â€ƒFirst smokeâ€‘test

Create tests/test_eye_integration.py:

def test_eye_ctd(tmp_path):
    data = \"\"\"@prefix : <http://ex/> .
              :plan a :PlannedAction ; :contains :CauseHarm .\"\"\"
    rules = \"\"\"@prefix : <http://ex/> .
               { ?p :contains :CauseHarm } => { ?p :violates :NoHarm } .
               { ?p :violates :NoHarm } => { ?p :obliges  :AlertStaff } .\"\"\"

    res = reason_over(jsonld=data, n3_rules=rules)
    g   = Graph().parse(data=res['patch'], format='turtle')
    assert (URIRef('http://ex/plan'), URIRef('http://ex/obliges'),
            URIRef('http://ex/AlertStaff')) in g

Run pytest -q â€“ it should pass in <â€¯300â€¯ms on a laptop.

â¸»

7â€ƒPerformance notes
	â€¢	eyeâ€‘js keeps the WASM engine in memory; the first call costs ~200â€¯ms for instantiation, subsequent calls ~5â€“10â€¯ms for thousands of triples.
	â€¢	For big rule sets (>â€¯1â€¯MB) set NODE_OPTIONS="--max-old-space-size=4096" or stream them in chunks.
	â€¢	Because EYE is forwardâ€‘chaining, it only returns new triples by default â€“ perfect for CogitareLinkâ€™s patch model.   ï¿¼

â¸»

8â€ƒExample agent call

{
  "name": "reason_over",
  "arguments": {
    "jsonld": "<events.jsonld>",          // already loaded doc
    "n3_rules": "<logic.n3>"              // full file text
  }
}

CogitareLink will:
	1.	Convert the JSONâ€‘LD to Turtle (jsonld_to_turtle() helper).
	2.	Feed data + rules into eyeâ€‘js.
	3.	Wrap the delta with prov:Activity.
	4.	Return the NL summary (â€œğŸ’¡â€¯EYE derived 14 triples via logic.n3â€).

â¸»

9â€ƒOptional: keep a JSON proof

Add --proof /dev/stdout inside js/eye-runner.mjs (supported since 18.16.1) and capture the JSON blob to store alongside the patch.  Perfect for audit trails.

â¸»

TL;DR
	1.	npm install eyereasoner@18.16.4  \n2. Add js/eye-runner.mjs using the n3reasoner APIÂ  ï¿¼  \n3. Patch reason.sandbox with _eye_run()  \n4. Extend reason_over spec with n3_rules  \n5. Write a smoke testâ€”done!

You now have a fullyâ€‘portable EYE reasoner running inside CogitareLink, still 100â€¯% dataâ€‘driven and provenanceâ€‘aware.